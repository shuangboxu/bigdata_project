# Project Structure Overview

## Project Overview

This project is a LangGraph-based credit risk analysis system that implements an end-to-end data analysis workflow using a multi-agent architecture. The project integrates traditional command-line interface (CLI) tools with the modern LangGraph agent framework, enabling exploratory data analysis, machine learning modeling, visualization, and automated report generation for credit risk data.

## Core Technology Stack

- **Python Data Science Stack**: pandas, numpy, scikit-learn, matplotlib
- **Agent Orchestration Framework**: LangGraph
- **Large Language Model Integration**: OpenAI API
- **Data Processing**: openpyxl (Excel), scipy
- **Template Engine**: Jinja2
- **Environment Management**: python-dotenv

## Directory Structure Details

### 1. `agent_flow/` - LangGraph Agent Modules

This is the core agent orchestration layer, containing the LangGraph-based multi-agent workflow implementation.

- **`planner.py`** - Data Planning Agent
  - Responsible for generating high-level execution plans
  - Defines data processing, feature engineering, model training, and evaluation steps
  - Passes the raw dataframe to subsequent nodes

- **`executor.py`** - Data Execution Agent
  - Executes actual data processing and model training tasks
  - Cleans data and handles missing values
  - Trains gradient boosting classifier
  - Collects model metrics and predictions

- **`visualizer.py`** - Visualization Agent
  - Generates visualization charts such as ROC curves
  - Saves charts to the `artifacts/classification/` directory

- **`reporter.py`** - Report Generation Agent
  - Uses OpenAI API to generate intelligent analysis reports (when API key is configured)
  - Provides fallback templates to ensure workflow always completes
  - Generates Markdown format analysis reports

- **`run_workflow.py`** - Workflow Runner
  - Integrates all agent nodes
  - Defines state graph and data flow between nodes
  - Generates HTML format comprehensive summary report (`reports/workflow_overview.html`)

- **`utils.py`** - Utility Functions
  - Provides data loading and common helper functions

### 2. `src/` - Core Data Science Modules

Contains reusable data processing and modeling code, called by agents and CLI tools.

- **`config.py`** - Configuration Management
  - Defines default input paths and global configuration parameters

- **`io_utils.py`** - Input/Output Tools
  - Implements robust data readers (supports Excel and CSV)
  - Handles automatic format recognition and fallback

- **`schema.py`** - Data Schema Definition
  - Defines data structures and type constraints

- **`preprocess.py`** - Data Preprocessing
  - Data cleaning and transformation
  - Missing value handling
  - Outlier detection

- **`features.py`** - Feature Engineering
  - Feature construction and transformation
  - Feature selection logic

- **`split.py`** - Data Splitting
  - Train/validation/test set splitting
  - Supports time-series aware splitting strategies

- **`eda.py`** - Exploratory Data Analysis
  - Generates data summary statistics
  - Creates distribution plots and correlation analysis

- **`pipeline_cls.py`** - Classification Modeling Pipeline
  - Implements complete classification model training workflow
  - Uses gradient boosting classifier to handle class imbalance
  - Generates confusion matrix and performance metrics

- **`pipeline_reg.py`** - Regression Modeling Pipeline
  - Implements regression model training workflow
  - Supports continuous variable prediction

- **`cluster.py`** - Clustering Analysis
  - Implements unsupervised learning algorithms
  - K-means clustering and silhouette analysis

- **`evaluate.py`** - Model Evaluation
  - Calculates various performance metrics (accuracy, F1, AUC, etc.)
  - Generates evaluation reports

- **`report.py`** - Report Compilation
  - Compiles analysis results into structured reports
  - Supports Markdown and HTML output

- **`templates/`** - Report Templates
  - Stores Jinja2 template files for report generation

### 3. `artifacts/` - Model and Results Storage

Persistent storage for all analysis artifacts.

- **`classification/`** - Classification Task Output
  - Trained classification models
  - ROC curve plots (`roc_curve_agent.png`)
  - Confusion matrix plots
  - Model performance metrics JSON files

- **`regression/`** - Regression Task Output
  - Regression models and predictions
  - Actual vs. predicted scatter plots

- **`clustering/`** - Clustering Task Output
  - Clustering models
  - Cluster silhouette plots and profile analysis

- **`eda/`** - Exploratory Analysis Output
  - Data distribution plots
  - Correlation heatmaps
  - Statistical summaries

### 4. `reports/` - Analysis Reports

Stores generated report documents.

- **`agent_llm_report.md`** - LLM-enhanced report generated by LangGraph agents
- **`workflow_overview.html`** - Workflow comprehensive summary (HTML format)
- **`report.md`** - Standard report generated by traditional CLI tools
- **`English_Project_Design_Report.md`** - English project design report

### 5. `data/` - Data Directory

Stores input datasets (this directory is typically excluded from version control).

- **`数据示例.xlsx`** - Sample dataset (Lending Club-style loan portfolio data)
  - Contains 10,722 loan records
  - 44 feature variables
  - Binary target variable (default/non-default)

### 6. Root Directory Files

- **`main.py`** - Traditional CLI Entry Point
  - Provides command-line interface to access functional modules
  - Supports independent execution of EDA, training, clustering, and other tasks
  - Does not depend on LangGraph framework

- **`llm_insights.py`** - LLM Insights Generation
  - Calls large language models to enhance report content
  - Optional feature, requires OpenAI API key configuration

- **`requirements.txt`** - Python Dependencies List
  - Lists all required third-party libraries and version requirements

- **`README.md`** - Project Documentation (English)
  - Environment setup instructions
  - Usage guides and examples
  - Troubleshooting guidance

- **`.env.example`** - Environment Variables Example
  - API key configuration template
  - Data path configuration examples

## Workflow Description

### LangGraph Agent Workflow

This is the recommended usage approach, providing end-to-end automation:

```bash
python -m agent_flow.run_workflow --data data/数据示例.xlsx
```

**Execution Flow**:
1. **Planner** → Generates execution plan and passes raw data
2. **Executor** → Cleans data, trains model, collects metrics
3. **Visualizer** → Generates ROC curves and other charts
4. **Reporter** → Generates Markdown analysis report
5. **HTML Summary** → Consolidates all results into an interactive webpage

**Key Outputs**:
- `artifacts/classification/roc_curve_agent.png`
- `reports/agent_llm_report.md`
- `reports/workflow_overview.html`

### Traditional CLI Tools

Standalone command-line tools suitable for executing specific tasks individually:

```bash
# Exploratory data analysis
python main.py eda --in data/数据示例.xlsx

# Supervised learning - Classification
python main.py train-cls --in data/数据示例.xlsx --target grade

# Supervised learning - Regression
python main.py train-reg --in data/数据示例.xlsx --target interestRate

# Unsupervised learning - Clustering
python main.py cluster --in data/数据示例.xlsx

# Generate static report bundle
python main.py report

# Use LLM-enhanced report (optional)
python main.py report-llm
```

## Data Flow and State Management

The LangGraph workflow uses `WorkflowState` TypedDict to pass state between nodes:

- **data**: Raw/cleaned pandas DataFrame
- **plan**: Execution plan (list of steps)
- **metrics**: Model performance metrics dictionary
- **model**: Trained model object
- **roc_path**: ROC curve plot save path
- **report_text**: Generated Markdown report text
- **html_path**: HTML summary file path

## Feature Highlights

### 1. Dual-Mode Operation
- **Agent Mode**: Fully automated end-to-end workflow
- **CLI Mode**: Flexible single-step execution control

### 2. Robust Data Processing
- Automatic detection and handling of binary classification target variables
- Intelligent missing value imputation strategies
- Handles severe class imbalance issues

### 3. Optional LLM Enhancement
- Gain intelligent analysis insights when OpenAI API key is configured
- Uses deterministic templates when unconfigured to ensure workflow integrity

### 4. Comprehensive Visualization
- ROC curves, confusion matrices
- Cluster silhouette plots
- Data distribution and correlation charts

### 5. Traceability
- All models and charts persistently stored
- Generates detailed HTML and Markdown reports
- Records execution plans and steps

## Technical Architecture Features

1. **Modular Design**: Core logic separated from orchestration layer, easy to maintain and extend
2. **Agent Collaboration**: Different role agents focus on their specific responsibilities
3. **State-Driven**: Uses typed state for safe data passing between agents
4. **Fault Tolerance**: Multi-layer fallback strategies ensure workflow robustness
5. **Extensibility**: Easy to add new agent nodes or replace model algorithms

## Use Cases

- **Credit Risk Modeling**: Assess loan default probability
- **Automated Reporting**: Generate auditable analysis documents
- **Model Experimentation**: Rapidly iterate different feature engineering and modeling strategies
- **Educational Demonstrations**: Showcase end-to-end machine learning workflows
- **Prototype Development**: Quickly validate ideas for production systems

## Environment Requirements

- Python 3.8+
- Virtual environment recommended
- Optional: OpenAI API key (for LLM enhancement features)

## Contribution and Extension

The project adopts a clear layered architecture, facilitating extensions:

- Add new agent nodes: Create new modules in `agent_flow/`
- Extend modeling capabilities: Add new pipelines in `src/`
- Enhance visualization: Extend `visualizer.py` or add new chart types
- Customize reports: Modify templates in `src/templates/`

## Summary

This project successfully combines traditional data science workflows with modern agent orchestration technology, providing a reproducible, auditable, and extensible credit risk analysis platform. Through LangGraph framework orchestration, it achieves fully automated workflows from data ingestion to report generation, while maintaining sufficient flexibility to support customization and extension needs.
